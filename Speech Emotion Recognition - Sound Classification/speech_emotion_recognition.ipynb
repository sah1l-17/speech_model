{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Trainer, TrainingArguments, Wav2Vec2ForSequenceClassification\n",
    "import warnings\n",
    "warnings. filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:21.769751Z",
     "iopub.status.busy": "2022-03-06T08:10:21.769219Z",
     "iopub.status.idle": "2022-03-06T08:10:21.828687Z",
     "shell.execute_reply": "2022-03-06T08:10:21.827926Z",
     "shell.execute_reply.started": "2022-03-06T08:10:21.769696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is Loaded\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "labels = []\n",
    "for dirname, _, filenames in os.walk('//Users/mac/Documents/speech_model/dataset'):\n",
    "    for filename in filenames:\n",
    "        paths.append(os.path.join(dirname, filename))\n",
    "        label = filename.split('_')[-1]\n",
    "        label = label.split('.')[0]\n",
    "        labels.append(label.lower())\n",
    "    if len(paths) == 2800:\n",
    "        break\n",
    "print('Dataset is Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:21.833767Z",
     "iopub.status.busy": "2022-03-06T08:10:21.833203Z",
     "iopub.status.idle": "2022-03-06T08:10:21.840633Z",
     "shell.execute_reply": "2022-03-06T08:10:21.839705Z",
     "shell.execute_reply.started": "2022-03-06T08:10:21.833723Z"
    }
   },
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:21.843337Z",
     "iopub.status.busy": "2022-03-06T08:10:21.842478Z",
     "iopub.status.idle": "2022-03-06T08:10:21.850424Z",
     "shell.execute_reply": "2022-03-06T08:10:21.849546Z",
     "shell.execute_reply.started": "2022-03-06T08:10:21.843287Z"
    }
   },
   "outputs": [],
   "source": [
    "paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:21.855065Z",
     "iopub.status.busy": "2022-03-06T08:10:21.852448Z",
     "iopub.status.idle": "2022-03-06T08:10:21.862887Z",
     "shell.execute_reply": "2022-03-06T08:10:21.861848Z",
     "shell.execute_reply.started": "2022-03-06T08:10:21.855021Z"
    }
   },
   "outputs": [],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:21.865397Z",
     "iopub.status.busy": "2022-03-06T08:10:21.864625Z",
     "iopub.status.idle": "2022-03-06T08:10:21.884478Z",
     "shell.execute_reply": "2022-03-06T08:10:21.883678Z",
     "shell.execute_reply.started": "2022-03-06T08:10:21.865339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>//Users/mac/Documents/speech_model/dataset/TES...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>//Users/mac/Documents/speech_model/dataset/TES...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>//Users/mac/Documents/speech_model/dataset/TES...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>//Users/mac/Documents/speech_model/dataset/TES...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>//Users/mac/Documents/speech_model/dataset/TES...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              speech    label\n",
       "0  //Users/mac/Documents/speech_model/dataset/TES...  disgust\n",
       "1  //Users/mac/Documents/speech_model/dataset/TES...  disgust\n",
       "2  //Users/mac/Documents/speech_model/dataset/TES...  disgust\n",
       "3  //Users/mac/Documents/speech_model/dataset/TES...  disgust\n",
       "4  //Users/mac/Documents/speech_model/dataset/TES...  disgust"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a dataframe\n",
    "df = pd.DataFrame()\n",
    "df['speech'] = paths\n",
    "df['label'] = labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:21.886673Z",
     "iopub.status.busy": "2022-03-06T08:10:21.886018Z",
     "iopub.status.idle": "2022-03-06T08:10:21.896515Z",
     "shell.execute_reply": "2022-03-06T08:10:21.895749Z",
     "shell.execute_reply.started": "2022-03-06T08:10:21.886625Z"
    }
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:21.898875Z",
     "iopub.status.busy": "2022-03-06T08:10:21.898327Z",
     "iopub.status.idle": "2022-03-06T08:10:22.220182Z",
     "shell.execute_reply": "2022-03-06T08:10:22.217494Z",
     "shell.execute_reply.started": "2022-03-06T08:10:21.898831Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:22.222260Z",
     "iopub.status.busy": "2022-03-06T08:10:22.221732Z",
     "iopub.status.idle": "2022-03-06T08:10:22.229609Z",
     "shell.execute_reply": "2022-03-06T08:10:22.228832Z",
     "shell.execute_reply.started": "2022-03-06T08:10:22.222219Z"
    }
   },
   "outputs": [],
   "source": [
    "def waveplot(data, sr, emotion):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.title(emotion, size=20)\n",
    "    librosa.display.waveshow(data, sr=sr)\n",
    "    plt.show()\n",
    "    \n",
    "def spectogram(data, sr, emotion):\n",
    "    x = librosa.stft(data)\n",
    "    xdb = librosa.amplitude_to_db(abs(x))\n",
    "    plt.figure(figsize=(11,4))\n",
    "    plt.title(emotion, size=20)\n",
    "    librosa.display.specshow(xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:22.231270Z",
     "iopub.status.busy": "2022-03-06T08:10:22.231004Z",
     "iopub.status.idle": "2022-03-06T08:10:23.146801Z",
     "shell.execute_reply": "2022-03-06T08:10:23.146097Z",
     "shell.execute_reply.started": "2022-03-06T08:10:22.231234Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion = 'fear'\n",
    "path = np.array(df['speech'][df['label']==emotion])[0]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "waveplot(data, sampling_rate, emotion)\n",
    "spectogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:23.155633Z",
     "iopub.status.busy": "2022-03-06T08:10:23.154991Z",
     "iopub.status.idle": "2022-03-06T08:10:23.994758Z",
     "shell.execute_reply": "2022-03-06T08:10:23.994066Z",
     "shell.execute_reply.started": "2022-03-06T08:10:23.155592Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion = 'angry'\n",
    "path = np.array(df['speech'][df['label']==emotion])[1]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "waveplot(data, sampling_rate, emotion)\n",
    "spectogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:23.996204Z",
     "iopub.status.busy": "2022-03-06T08:10:23.995831Z",
     "iopub.status.idle": "2022-03-06T08:10:24.586458Z",
     "shell.execute_reply": "2022-03-06T08:10:24.585772Z",
     "shell.execute_reply.started": "2022-03-06T08:10:23.996169Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion = 'disgust'\n",
    "path = np.array(df['speech'][df['label']==emotion])[0]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "waveplot(data, sampling_rate, emotion)\n",
    "spectogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:24.588549Z",
     "iopub.status.busy": "2022-03-06T08:10:24.587693Z",
     "iopub.status.idle": "2022-03-06T08:10:25.238414Z",
     "shell.execute_reply": "2022-03-06T08:10:25.237736Z",
     "shell.execute_reply.started": "2022-03-06T08:10:24.588508Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion = 'neutral'\n",
    "path = np.array(df['speech'][df['label']==emotion])[0]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "waveplot(data, sampling_rate, emotion)\n",
    "spectogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:25.240111Z",
     "iopub.status.busy": "2022-03-06T08:10:25.239588Z",
     "iopub.status.idle": "2022-03-06T08:10:25.855499Z",
     "shell.execute_reply": "2022-03-06T08:10:25.854777Z",
     "shell.execute_reply.started": "2022-03-06T08:10:25.240068Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion = 'sad'\n",
    "path = np.array(df['speech'][df['label']==emotion])[0]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "waveplot(data, sampling_rate, emotion)\n",
    "spectogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:25.857568Z",
     "iopub.status.busy": "2022-03-06T08:10:25.857041Z",
     "iopub.status.idle": "2022-03-06T08:10:26.546774Z",
     "shell.execute_reply": "2022-03-06T08:10:26.546095Z",
     "shell.execute_reply.started": "2022-03-06T08:10:25.857525Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion = 'ps'\n",
    "path = np.array(df['speech'][df['label']==emotion])[0]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "waveplot(data, sampling_rate, emotion)\n",
    "spectogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:26.548488Z",
     "iopub.status.busy": "2022-03-06T08:10:26.548059Z",
     "iopub.status.idle": "2022-03-06T08:10:27.205424Z",
     "shell.execute_reply": "2022-03-06T08:10:27.204756Z",
     "shell.execute_reply.started": "2022-03-06T08:10:26.548422Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion = 'happy'\n",
    "path = np.array(df['speech'][df['label']==emotion])[0]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "waveplot(data, sampling_rate, emotion)\n",
    "spectogram(data, sampling_rate, emotion)\n",
    "Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>//Users/mac/Documents/speech_model/dataset/TES...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>//Users/mac/Documents/speech_model/dataset/TES...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              speech  label\n",
       "0  //Users/mac/Documents/speech_model/dataset/TES...      0\n",
       "1  //Users/mac/Documents/speech_model/dataset/TES...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert labels to integers\n",
    "label_map = {label: idx for idx, label in enumerate(df['label'].unique())}\n",
    "inverse_label_map = {idx: label for label, idx in label_map.items()}\n",
    "df['label'] = df['label'].map(label_map)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:27.207267Z",
     "iopub.status.busy": "2022-03-06T08:10:27.206806Z",
     "iopub.status.idle": "2022-03-06T08:10:27.212261Z",
     "shell.execute_reply": "2022-03-06T08:10:27.211579Z",
     "shell.execute_reply.started": "2022-03-06T08:10:27.207228Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpeechEmotionDataset(Dataset):\n",
    "    def __init__(self, df, processor, max_length=32000):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the file path and label\n",
    "        audio_path = self.df.iloc[idx]['speech']  # This is the path to the audio file\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        # Load the audio file\n",
    "        speech, sr = librosa.load(audio_path, sr = 16000)\n",
    "        \n",
    "\n",
    "        # Pad or truncate the speech to the required length\n",
    "        if len(speech) > self.max_length:\n",
    "            speech = speech[:self.max_length]\n",
    "        else:\n",
    "            speech = np.pad(speech, (0, self.max_length - len(speech)), 'constant')\n",
    "\n",
    "        # Preprocess the audio data using the processor\n",
    "        inputs = self.processor(speech, sampling_rate=16000, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length)\n",
    "        input_values = inputs.input_values.squeeze()\n",
    "\n",
    "        return {'input_values': input_values, 'label': torch.tensor(label, dtype=torch.long)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:27.214030Z",
     "iopub.status.busy": "2022-03-06T08:10:27.213572Z",
     "iopub.status.idle": "2022-03-06T08:10:27.251263Z",
     "shell.execute_reply": "2022-03-06T08:10:27.250541Z",
     "shell.execute_reply.started": "2022-03-06T08:10:27.213993Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the processor and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:10:27.257564Z",
     "iopub.status.busy": "2022-03-06T08:10:27.252793Z",
     "iopub.status.idle": "2022-03-06T08:14:05.042142Z",
     "shell.execute_reply": "2022-03-06T08:14:05.041249Z",
     "shell.execute_reply.started": "2022-03-06T08:10:27.257519Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-base\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, num_labels=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T08:36:07.967589Z",
     "iopub.status.busy": "2022-03-06T08:36:07.967321Z",
     "iopub.status.idle": "2022-03-06T08:36:08.225315Z",
     "shell.execute_reply": "2022-03-06T08:36:08.224578Z",
     "shell.execute_reply.started": "2022-03-06T08:36:07.967540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = SpeechEmotionDataset(train_df, processor)\n",
    "test_dataset = SpeechEmotionDataset(test_df, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"//Users/mac/Documents/speech_model/results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01,\n",
    "    report_to = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ =precision_recall_fscore_support(labels, preds, average = 'weighted')\n",
    "    return {\n",
    "        \"accuracy\" : accuracy,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1\" : f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Load the processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"/Users/mac/Documents/speech_model/trained_model\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"/Users/mac/Documents/speech_model/trained_model\")\n",
    "\n",
    "print(\"Model and processor loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_path):\n",
    "    # Load the audio file\n",
    "    speech, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    # Preprocess the audio file\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Forward pass to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Get the predicted label (index of the highest logit value)\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: angry\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"/Users/mac/Documents/speech_model/dataset/TESS Toronto emotional speech set data/YAF_angry/YAF_base_angry.wav\"  # Provide the path to your local audio file\n",
    "predicted_emotion = predict_emotion(audio_path)\n",
    "\n",
    "\n",
    "emotion_mapping = {\n",
    "    0: \"fear\",\n",
    "    1: \"angry\",\n",
    "    2: \"disgst\",\n",
    "    3: \"netural\",\n",
    "    4: \"sad\",\n",
    "    5: \"pleasant surprise\",\n",
    "    6: \"happy\"\n",
    "}\n",
    "\n",
    "print(f\"Predicted emotion: {emotion_mapping[predicted_emotion]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Record live audio function with manual input control\n",
    "def record_audio(sample_rate=16000):\n",
    "    recorded_audio = []\n",
    "    is_recording = False\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Enter 's' to start/resume, 'p' to pause, 'e' to end: \").strip().lower()\n",
    "        \n",
    "        if user_input == 's':\n",
    "            if not is_recording:\n",
    "                print(\"Recording started. Enter 'p' to pause, 'e' to end.\")\n",
    "                is_recording = True\n",
    "            audio_chunk = sd.rec(int(1 * sample_rate), samplerate=sample_rate, channels=1, dtype=\"float32\")\n",
    "            sd.wait()\n",
    "            recorded_audio.append(audio_chunk.flatten())\n",
    "            \n",
    "        elif user_input == 'p':\n",
    "            if is_recording:\n",
    "                print(\"Recording paused. Enter 's' to resume or 'e' to end.\")\n",
    "                is_recording = False\n",
    "            \n",
    "        elif user_input == 'e':\n",
    "            print(\"Recording ended.\")\n",
    "            break\n",
    "    \n",
    "    # Concatenate all audio chunks into one array\n",
    "    return np.concatenate(recorded_audio)\n",
    "\n",
    "# Split audio into chunks of a specified duration\n",
    "def split_audio(audio, sample_rate=16000, chunk_duration=3):\n",
    "    chunk_size = int(chunk_duration * sample_rate)\n",
    "    num_chunks = len(audio) // chunk_size\n",
    "    # Handle the case where the length is shorter than one chunk\n",
    "    if len(audio) < chunk_size:\n",
    "        print(f\"Not enough audio data to create a chunk of {chunk_duration} seconds.\")\n",
    "        return [audio]  # Return as a single chunk if not enough data for multiple chunks\n",
    "    print(f\"Audio length: {len(audio)}, Chunk size: {chunk_size}, Number of chunks: {num_chunks}\")\n",
    "    return [audio[i*chunk_size : (i+1)*chunk_size] for i in range(num_chunks)]\n",
    "\n",
    "# Predict emotion for each chunk\n",
    "def predict_emotion_for_chunk(chunk, processor, model, sample_rate=16000):\n",
    "    # Save chunk to a temporary file using soundfile\n",
    "    temp_path = \"temp_chunk.wav\"\n",
    "    sf.write(temp_path, chunk, sample_rate)\n",
    "    \n",
    "    # Use the provided predict_emotion function\n",
    "    predicted_label = predict_emotion(temp_path)\n",
    "    \n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_path)\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Main function for emotion prediction from live audio\n",
    "def predict_emotion_from_live_audio(processor, model, sample_rate=16000, chunk_duration=3):\n",
    "    # Record live audio with user-controlled start, pause, and stop\n",
    "    live_audio = record_audio(sample_rate=sample_rate)\n",
    "    \n",
    "    # Print length of recorded audio\n",
    "    print(f\"Length of recorded audio: {len(live_audio)} samples\")\n",
    "\n",
    "    # Split audio into chunks of chunk_duration seconds\n",
    "    chunks = split_audio(live_audio, sample_rate=sample_rate, chunk_duration=chunk_duration)\n",
    "    \n",
    "    # Emotion mapping (ensure your model's label mapping matches this)\n",
    "    emotion_mapping = {\n",
    "        0: \"happy\",\n",
    "        1: \"sad\",\n",
    "        2: \"angry\",\n",
    "        3: \"fearful\",\n",
    "        4: \"neutral\",\n",
    "        5: \"disgust\",\n",
    "        6: \"surprise\"\n",
    "    }\n",
    "    \n",
    "    # Predict emotion for each chunk and accumulate results\n",
    "    predictions = []\n",
    "    for chunk in chunks:\n",
    "        print(\"Processing next chunk...\")\n",
    "        if len(chunk) == 0:\n",
    "            print(\"Empty chunk detected, skipping.\")\n",
    "            continue\n",
    "        predicted_label = predict_emotion_for_chunk(chunk, processor, model, sample_rate=sample_rate)\n",
    "        predictions.append(emotion_mapping[predicted_label])\n",
    "    \n",
    "    # Count occurrences of each emotion\n",
    "    emotion_counts = Counter(predictions)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_chunks = len(predictions)\n",
    "    if total_chunks > 0:\n",
    "        emotion_percentages = {emotion: (count / total_chunks) * 100 for emotion, count in emotion_counts.items()}\n",
    "    \n",
    "        # Print the final result\n",
    "        print(\"Predicted emotion distribution:\")\n",
    "        for emotion, percentage in emotion_percentages.items():\n",
    "            print(emotion + \": \" + str(round(percentage, 2)) + \"%\")\n",
    "    else:\n",
    "        print(\"No audio chunks were processed.\")\n",
    "\n",
    "# Use the provided predict_emotion function\n",
    "def predict_emotion(audio_path):\n",
    "    # Load the audio file\n",
    "    speech, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    # Preprocess the audio file\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Forward pass to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Get the predicted label (index of the highest logit value)\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "# Assuming the processor and model are already loaded in the notebook\n",
    "predict_emotion_from_live_audio(processor, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
